<!DOCTYPE html><html lang="zh"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta name="renderer" content="webkit"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=no,email=no,adress=no"><meta name="theme-color" content="#000000"><meta http-equiv="window-target" content="_top"><title>Spark-使用文档 | Hexo</title><meta name="description" content="Apache Spark是一个围绕速度、易用性和复杂分析构建的大数据处理框架。"><meta property="og:type" content="article"><meta property="og:title" content="Spark-使用文档"><meta property="og:url" content="http://fansichao.github.com/blog/Tools/BigData/bigdata-spark.md/index.html"><meta property="og:site_name" content="Fansichao Blog"><meta property="og:description" content="Apache Spark是一个围绕速度、易用性和复杂分析构建的大数据处理框架。"><meta property="og:locale"><meta property="og:image" content="https://images2015.cnblogs.com/blog/1004194/201608/1004194-20160830094200918-1846127221.png"><meta property="og:image" content="https://images2015.cnblogs.com/blog/1004194/201608/1004194-20160829161404996-1972748563.png"><meta property="og:image" content="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/images/image002.jpg"><meta property="article:published_time" content="2021-10-23T11:54:51.159Z"><meta property="article:modified_time" content="2021-10-23T11:54:51.160Z"><meta property="article:author" content="Fansichao"><meta property="article:tag" content="bigdata"><meta property="article:tag" content="spark"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://images2015.cnblogs.com/blog/1004194/201608/1004194-20160830094200918-1846127221.png"><link rel="canonical" href="http://fansichao.github.com/blog/Tools/BigData/bigdata-spark.md/index.html"><link rel="alternate" href="/atom.xml" title="Fansichao Blog" type="application/atom+xml"><link rel="icon" href="/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/blog/css/style.css"><link href="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.3.5/dist/jquery.fancybox.min.css" rel="stylesheet"><meta name="generator" content="Hexo 5.4.0"></head><body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="slimContent"><div class="navbar-header"><div class="profile-block text-center"><a id="avatar" href="https://github.com/fansichao" target="_blank"><img class="img-circle img-rotate" src="/blog/images/avatar.jpg" width="200" height="200"></a><h2 id="name" class="hidden-xs hidden-sm">fansichao</h2><h3 id="title" class="hidden-xs hidden-sm hidden-md">Data Engineer &amp; SSE &amp; CYO</h3><small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Shenzhen, China</small></div><div class="search" id="search-form-wrap"><form class="search-form sidebar-form"><div class="input-group"><input type="text" class="search-form-input form-control" placeholder="Search"> <span class="input-group-btn"><button type="submit" class="search-form-submit btn btn-flat" onclick="return!1"><i class="icon icon-search"></i></button></span></div></form><div class="ins-search"><div class="ins-search-mask"></div><div class="ins-search-container"><div class="ins-input-wrapper"><input type="text" class="ins-search-input" placeholder="Type something..." x-webkit-speech> <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div><div class="ins-section-wrapper"><div class="ins-section-container"></div></div></div></div></div><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false"><span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span></button></div><nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation"><ul class="nav navbar-nav main-nav"><li class="menu-item menu-item-home"><a href="/blog/."><i class="icon icon-home-fill"></i> <span class="menu-title">Home</span></a></li><li class="menu-item menu-item-archives"><a href="/blog/archives"><i class="icon icon-archives-fill"></i> <span class="menu-title">Archives</span></a></li><li class="menu-item menu-item-categories"><a href="/blog/categories"><i class="icon icon-folder"></i> <span class="menu-title">Categories</span></a></li><li class="menu-item menu-item-tags"><a href="/blog/tags"><i class="icon icon-tags"></i> <span class="menu-title">Tags</span></a></li><li class="menu-item menu-item-repository"><a href="/blog/repository"><i class="icon icon-project"></i> <span class="menu-title">Repository</span></a></li><li class="menu-item menu-item-books"><a href="/blog/books"><i class="icon icon-book-fill"></i> <span class="menu-title">Books</span></a></li><li class="menu-item menu-item-links"><a href="/blog/links"><i class="icon icon-friendship"></i> <span class="menu-title">Links</span></a></li><li class="menu-item menu-item-about"><a href="/blog/about"><i class="icon icon-cup-fill"></i> <span class="menu-title">About</span></a></li></ul><ul class="social-links"><li><a href="https://github.com/cofess" target="_blank" title="Github" data-toggle="tooltip" data-placement="top"><i class="icon icon-github"></i></a></li><li><a href="http://weibo.com/cofess" target="_blank" title="Weibo" data-toggle="tooltip" data-placement="top"><i class="icon icon-weibo"></i></a></li><li><a href="https://twitter.com/iwebued" target="_blank" title="Twitter" data-toggle="tooltip" data-placement="top"><i class="icon icon-twitter"></i></a></li><li><a href="https://www.behance.net/cofess" target="_blank" title="Behance" data-toggle="tooltip" data-placement="top"><i class="icon icon-behance"></i></a></li><li><a href="/blog/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul></nav></div></header><aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar"><div class="slimContent"><div class="widget"><h3 class="widget-title">Board</h3><div class="widget-body"><div id="board"><div class="content"><p>欢迎交流与分享经验!</p></div></div></div></div><div class="widget"><h3 class="widget-title">Categories</h3><div class="widget-body"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/blog/categories/bigdata/">bigdata</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/linux/">linux</a><span class="category-list-count">16</span></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/module/">module</a><span class="category-list-count">41</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/blog/categories/module/docker/">docker</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/module/graph/">graph</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/module/grpah/">grpah</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/module/hexo/">hexo</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/python/">python</a><span class="category-list-count">26</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/blog/categories/python/data-structure/">data-structure</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/python/python-algorithm/">python-algorithm</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/python/python-%E5%86%85%E7%BD%AE%E5%8C%85/">python-内置包</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/python/python-%E5%9F%BA%E7%A1%80/">python-基础</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/python/python-%E8%BF%9B%E9%98%B6/">python-进阶</a><span class="category-list-count">13</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/web/">web</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/xxx/">xxx</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><span class="category-list-count">2</span></li></ul></div></div><div class="widget"><h3 class="widget-title">Tags</h3><div class="widget-body"><ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/Database/" rel="tag">Database</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/Graph/" rel="tag">Graph</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/Neo4j/" rel="tag">Neo4j</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/aaa/" rel="tag">aaa</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/bigdata/" rel="tag">bigdata</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/blog/" rel="tag">blog</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/centos8/" rel="tag">centos8</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/data-structure/" rel="tag">data-structure</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/docker/" rel="tag">docker</a><span class="tag-list-count">11</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/graph/" rel="tag">graph</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/gremlin/" rel="tag">gremlin</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/hadoop/" rel="tag">hadoop</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/hbase/" rel="tag">hbase</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/hexo/" rel="tag">hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/hugegraph/" rel="tag">hugegraph</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/index/" rel="tag">index</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/linux/" rel="tag">linux</a><span class="tag-list-count">16</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/module/" rel="tag">module</a><span class="tag-list-count">43</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/python/" rel="tag">python</a><span class="tag-list-count">26</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/python-algorithm/" rel="tag">python-algorithm</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/python-%E5%86%85%E7%BD%AE%E5%8C%85/" rel="tag">python-内置包</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/python-%E5%9F%BA%E7%A1%80/" rel="tag">python-基础</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/python-%E8%BF%9B%E9%98%B6/" rel="tag">python-进阶</a><span class="tag-list-count">13</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/scylladb/" rel="tag">scylladb</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/shell/" rel="tag">shell</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/spark/" rel="tag">spark</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/todo/" rel="tag">todo</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/web/" rel="tag">web</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/" rel="tag">消息队列</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E7%BC%93%E5%AD%98%E6%9C%BA%E5%88%B6/" rel="tag">缓存机制</a><span class="tag-list-count">1</span></li></ul></div></div><div class="widget"><h3 class="widget-title">Tag Cloud</h3><div class="widget-body tagcloud"><a href="/blog/tags/Database/" style="font-size:13.4px">Database</a> <a href="/blog/tags/Graph/" style="font-size:13.1px">Graph</a> <a href="/blog/tags/Neo4j/" style="font-size:13.4px">Neo4j</a> <a href="/blog/tags/aaa/" style="font-size:13px">aaa</a> <a href="/blog/tags/bigdata/" style="font-size:13.3px">bigdata</a> <a href="/blog/tags/blog/" style="font-size:13px">blog</a> <a href="/blog/tags/centos8/" style="font-size:13px">centos8</a> <a href="/blog/tags/data-structure/" style="font-size:13px">data-structure</a> <a href="/blog/tags/docker/" style="font-size:13.6px">docker</a> <a href="/blog/tags/graph/" style="font-size:13px">graph</a> <a href="/blog/tags/gremlin/" style="font-size:13px">gremlin</a> <a href="/blog/tags/hadoop/" style="font-size:13px">hadoop</a> <a href="/blog/tags/hbase/" style="font-size:13px">hbase</a> <a href="/blog/tags/hexo/" style="font-size:13px">hexo</a> <a href="/blog/tags/hugegraph/" style="font-size:13.1px">hugegraph</a> <a href="/blog/tags/index/" style="font-size:13.1px">index</a> <a href="/blog/tags/linux/" style="font-size:13.8px">linux</a> <a href="/blog/tags/module/" style="font-size:14px">module</a> <a href="/blog/tags/python/" style="font-size:13.9px">python</a> <a href="/blog/tags/python-algorithm/" style="font-size:13px">python-algorithm</a> <a href="/blog/tags/python-%E5%86%85%E7%BD%AE%E5%8C%85/" style="font-size:13.3px">python-内置包</a> <a href="/blog/tags/python-%E5%9F%BA%E7%A1%80/" style="font-size:13.5px">python-基础</a> <a href="/blog/tags/python-%E8%BF%9B%E9%98%B6/" style="font-size:13.7px">python-进阶</a> <a href="/blog/tags/scylladb/" style="font-size:13px">scylladb</a> <a href="/blog/tags/shell/" style="font-size:13px">shell</a> <a href="/blog/tags/spark/" style="font-size:13px">spark</a> <a href="/blog/tags/todo/" style="font-size:13.2px">todo</a> <a href="/blog/tags/web/" style="font-size:13.3px">web</a> <a href="/blog/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size:13.1px">大数据</a> <a href="/blog/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/" style="font-size:13px">消息队列</a> <a href="/blog/tags/%E7%BC%93%E5%AD%98%E6%9C%BA%E5%88%B6/" style="font-size:13px">缓存机制</a></div></div><div class="widget"><h3 class="widget-title">Archive</h3><div class="widget-body"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2021/10/">October 2021</a><span class="archive-list-count">244</span></li></ul></div></div><div class="widget"><h3 class="widget-title">Recent Posts</h3><div class="widget-body"><ul class="recent-post-list list-unstyled no-thumbnail"><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/blog/categories/xxx/">xxx</a></p><p class="item-title"><a href="/blog/aaa.md/" class="title">aaa</a></p><p class="item-date"><time datetime="2021-10-24T03:43:28.000Z" itemprop="datePublished">2021-10-24</time></p></div></li><li><div class="item-inner"><p class="item-category"></p><p class="item-title"><a href="/blog/resources.md/" class="title">(no title)</a></p><p class="item-date"><time datetime="2021-10-23T11:54:51.339Z" itemprop="datePublished">2021-10-23</time></p></div></li><li><div class="item-inner"><p class="item-category"></p><p class="item-title"><a href="/blog/Tools/ml/google-course/readme.md/" class="title">Google 机器学习速成课程</a></p><p class="item-date"><time datetime="2021-10-23T11:54:51.339Z" itemprop="datePublished">2021-10-23</time></p></div></li><li><div class="item-inner"><p class="item-category"></p><p class="item-title"><a href="/blog/Tools/ml/google-course/03%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B.md/" class="title">(no title)</a></p><p class="item-date"><time datetime="2021-10-23T11:54:51.288Z" itemprop="datePublished">2021-10-23</time></p></div></li><li><div class="item-inner"><p class="item-category"></p><p class="item-title"><a href="/blog/Tools/ml/google-course/04%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E5%9C%A8%E7%8E%B0%E5%AE%9E%E4%B8%96%E7%95%8C%E9%87%8C%E7%9A%84%E5%BA%94%E7%94%A8.md/" class="title">(no title)</a></p><p class="item-date"><time datetime="2021-10-23T11:54:51.288Z" itemprop="datePublished">2021-10-23</time></p></div></li></ul></div></div></div></aside><main class="main" role="main"><div class="content"><article id="post-Tools/BigData/bigdata-spark" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting"><div class="article-header"><h1 class="article-title" itemprop="name">Spark-使用文档</h1><div class="article-meta"><span class="article-date"><i class="icon icon-calendar-check"></i> <a href="/blog/Tools/BigData/bigdata-spark.md/" class="article-date"><time datetime="2021-10-23T11:54:51.159Z" itemprop="datePublished">2021-10-23</time> </a></span><span class="article-category"><i class="icon icon-folder"></i> <a class="article-category-link" href="/blog/categories/bigdata/">bigdata</a> </span><span class="article-tag"><i class="icon icon-tags"></i> <a class="article-tag-link-link" href="/blog/tags/bigdata/" rel="tag">bigdata</a>, <a class="article-tag-link-link" href="/blog/tags/spark/" rel="tag">spark</a> </span><span class="post-comment"><i class="icon icon-comment"></i> <a href="/blog/Tools/BigData/bigdata-spark.md/#comments" class="article-comment-link">Comments</a></span></div></div><div class="article-entry marked-body" itemprop="articleBody"><h2 id="1-1-Spark-说明"><a href="#1-1-Spark-说明" class="headerlink" title="1.1. Spark 说明"></a>1.1. Spark 说明</h2><p>Apache Spark 是一个围绕速度、易用性和复杂分析构建的大数据处理框架，最初在 2009 年由加州大学伯克利分校的 AMPLab 开发，并于 2010 年成为 Apache 的开源项目之一，与 Hadoop 和 Storm 等其他大数据和 MapReduce 技术相比，Spark 有如下优势：</p><ul><li>Spark 提供了一个全面、统一的框架用于管理各种有着不同性质(文本数据、图表数据等)的数据集和数据源(批量数据或实时的流数据)的大数据处理的需求</li><li>官方资料介绍 Spark 可以将 Hadoop 集群中的应用在内存中的运行速度提升 100 倍，甚至能够将应用在磁盘上的运行速度提升 10 倍</li></ul><p><a target="_blank" rel="noopener" href="http://spark.apache.org/">Spark 官网</a></p><h2 id="1-2-Spark-框架"><a href="#1-2-Spark-框架" class="headerlink" title="1.2. Spark 框架"></a>1.2. Spark 框架</h2><p><strong>spark 运行流程图</strong></p><p><img src="https://images2015.cnblogs.com/blog/1004194/201608/1004194-20160830094200918-1846127221.png" alt="Spark运行流程图"></p><p><strong>Spark 架构</strong></p><p><img src="https://images2015.cnblogs.com/blog/1004194/201608/1004194-20160829161404996-1972748563.png" alt="Spark架构"></p><p><strong>详见参考链接</strong>：<a target="_blank" rel="noopener" href="https://blog.csdn.net/swing2008/article/details/60869183">https://blog.csdn.net/swing2008/article/details/60869183</a></p><h2 id="1-3-Hadoop-说明"><a href="#1-3-Hadoop-说明" class="headerlink" title="1.3. Hadoop 说明"></a>1.3. Hadoop 说明</h2><p>Hadoop 是一个由 Apache 基金会所开发的<strong>分布式系统基础架构</strong>。</p><p>Hadoop 实现了一个分布式文件系统(Hadoop Distributed File System)，简称 HDFS。</p><p>Hadoop 的框架最核心的设计就是：HDFS 和 MapReduce。HDFS 为海量的数据提供了存储，而 MapReduce 则为海量的数据提供了计算</p><p><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/Hadoop/3526507?fr=aladdin">Hadoop 百度百科</a></p><p><strong>Hadoop 优点:</strong></p><ul><li>高可靠性。Hadoop 按位存储和处理数据的能力值得人们信赖。</li><li>高扩展性。Hadoop 是在可用的计算机集簇间分配数据并完成计算任务的，这些集簇可以方便地扩展到数以千计的节点中。</li><li>高效性。Hadoop 能够在节点之间动态地移动数据，并保证各个节点的动态平衡，因此处理速度非常快。</li><li>高容错性。Hadoop 能够自动保存数据的多个副本，并且能够自动将失败的任务重新分配。</li><li>低成本。与一体机、商用数据仓库以及 QlikView、Yonghong Z-Suite 等数据集市相比，hadoop 是开源的，项目的软件成本因此会大大降低。</li></ul><p><strong>Hadoop 框架</strong><br>Hadoop 有两个核心模块，<strong>分布式存储模块 HDFS</strong>和<strong>分布式计算模块 Mapreduce</strong>.</p><h2 id="1-4-Yarn-框架说明"><a href="#1-4-Yarn-框架说明" class="headerlink" title="1.4. Yarn 框架说明"></a>1.4. Yarn 框架说明</h2><p>由于原有框架 JobTracker/TaskTracker 需要大规模的调整来修复它在可扩展性，内存消耗，线程模型，可靠性和性能上的缺陷,所以推出了 Yarn 框架。</p><ul><li>参考链接：<a target="_blank" rel="noopener" href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/">https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/</a></li></ul><p>Yarn 框架核心在于将资源管理和任务调度/监控拆分。</p><ul><li>资源管理器: 全局管理所有应用程序计算资源的分配</li><li>每一个应用的 ApplicationMaster 负责相应的调度和协调</li><li>ResourceManager 和每一台机器的节点管理服务器能够管理用户在那台机器上的进程并能对计算进行组织</li></ul><p><img src="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/images/image002.jpg" alt="Hadoop新MapReduce框架Yarn"></p><ul><li>ResourceManager :中心服务，调度和启动 Job 中的 ApplicationMaster,并监控 ApplicationMaster 存在情况</li><li>NodeManager : 负责 Container 状态的维护，并向 ResourceManager 返回日志</li><li>ApplicationMaster :负责一个 Job 生命周期内的所有工作</li></ul><h1 id="2-Spark-环境部署"><a href="#2-Spark-环境部署" class="headerlink" title="2. Spark 环境部署"></a>2. Spark 环境部署</h1><p><strong>不同部署模式</strong></p><ul><li>Standalone 模式：独立部署模式</li><li>Apache Mesos</li><li>Hadoop YARN</li><li>Kubernetes</li></ul><p>版本说明:</p><ul><li>Spark 2.4.0</li><li>Scala 2.12</li><li>Spark 和 Hadoop 版本必须相互配合</li></ul><h2 id="2-1-安装-Scala"><a href="#2-1-安装-Scala" class="headerlink" title="2.1. 安装 Scala"></a>2.1. 安装 Scala</h2><p>Spark 支持 Scala、Java 和 Python 等语言，不过 Spark 是采用 Scala 语言开发，所以必须先安装 Scala.</p><p><strong>步骤 1：下载</strong><br>Scala-2.12.7 下载地址</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://downloads.lightbend.com/scala/2.12.7/scala-2.12.7.tgz</span><br></pre></td></tr></table></figure><p><strong>步骤 2：解压</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建目录</span></span><br><span class="line">tar -zxvf scala-2.12.7.tgz</span><br><span class="line">sudo mv scala-2.12.7 /usr/<span class="built_in">local</span>/scala</span><br><span class="line">sudo chown scfan:scfan -R /usr/<span class="built_in">local</span>/scala</span><br></pre></td></tr></table></figure><p><strong>步骤 3：配置环境变量</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打开文件</span></span><br><span class="line">sudo vim /etc/profile</span><br><span class="line"><span class="comment"># 添加内容如下</span></span><br><span class="line">export SCALA_HOME=/usr/local/scala</span><br><span class="line">export PATH=$SCALA_HOME/<span class="built_in">bin</span>:$PATH</span><br></pre></td></tr></table></figure><p><strong>步骤 4：生效与验证</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(env) [scfan@WOM ~]$ <span class="built_in">source</span> /etc/profile</span><br><span class="line">(env) [scfan@WOM ~]$ scala</span><br><span class="line">Welcome to Scala 2.12.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_171).</span><br><span class="line">Type <span class="keyword">in</span> expressions <span class="keyword">for</span> evaluation. Or try :<span class="built_in">help</span>.</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure><h2 id="2-2-安装-Spark"><a href="#2-2-安装-Spark" class="headerlink" title="2.2. 安装 Spark"></a>2.2. 安装 Spark</h2><p><strong>步骤 1：下载</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz</span><br></pre></td></tr></table></figure><p><strong>步骤 2：解压</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf spark-2.4.0-bin-hadoop2.7.tgz</span><br><span class="line">sudo mv spark-2.4.0-bin-hadoop2.7 /usr/<span class="built_in">local</span>/spark</span><br><span class="line">sudo chown -R scfan:scfan /usr/<span class="built_in">local</span>/spark</span><br></pre></td></tr></table></figure><p><strong>步骤 3：配置环境变量</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打开文件</span></span><br><span class="line">sudo vim /etc/profile</span><br><span class="line"><span class="comment"># 添加内容如下</span></span><br><span class="line"><span class="comment"># Spark path</span></span><br><span class="line">export SPARK_HOME=/usr/local/spark</span><br><span class="line">export PATH=$SPARK_HOME/<span class="built_in">bin</span>:$PATH</span><br></pre></td></tr></table></figure><p><strong>步骤 4：生效与验证</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">(env) [scfan@WOM ~]$ <span class="built_in">source</span> /etc/profile</span><br><span class="line">(env) [scfan@WOM spark]$ <span class="built_in">source</span> /etc/profile</span><br><span class="line">(env) [scfan@WOM spark]$ pyspark</span><br><span class="line">Python 2.7.11 (default, Apr 10 2018, 16:42:22)</span><br><span class="line">[GCC 4.4.7 20120313 (Red Hat 4.4.7-18)] on linux2</span><br><span class="line">Type <span class="string">&quot;help&quot;</span>, <span class="string">&quot;copyright&quot;</span>, <span class="string">&quot;credits&quot;</span> or <span class="string">&quot;license&quot;</span> <span class="keyword">for</span> more information.</span><br><span class="line">2018-12-06 15:37:54 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">Setting default <span class="built_in">log</span> level to <span class="string">&quot;WARN&quot;</span>.</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  <span class="string">&#x27;_/</span></span><br><span class="line"><span class="string">   /__ / .__/\_,_/_/ /_/\_\   version 2.4.0</span></span><br><span class="line"><span class="string">      /_/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Using Python version 2.7.11 (default, Apr 10 2018 16:42:22)</span></span><br><span class="line"><span class="string">SparkSession available as &#x27;</span>spark<span class="string">&#x27;.</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;</span></span><br></pre></td></tr></table></figure><p><strong>步骤 5：启动 Spark</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">(env) [scfan@WOM spark]$ ./bin/spark-shell --master <span class="built_in">local</span>[2]</span><br><span class="line">2018-12-06 15:49:10 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">Setting default <span class="built_in">log</span> level to <span class="string">&quot;WARN&quot;</span>.</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class="line">Spark context Web UI available at http://WOM:4040</span><br><span class="line">Spark context available as <span class="string">&#x27;sc&#x27;</span> (master = <span class="built_in">local</span>[2], app id = local-1544082590634).</span><br><span class="line">Spark session available as <span class="string">&#x27;spark&#x27;</span>.</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  <span class="string">&#x27;_/</span></span><br><span class="line"><span class="string">   /___/ .__/\_,_/_/ /_/\_\   version 2.4.0</span></span><br><span class="line"><span class="string">      /_/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_171)</span></span><br><span class="line"><span class="string">Type in expressions to have them evaluated.</span></span><br><span class="line"><span class="string">Type :help for more information.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">scala&gt;</span></span><br></pre></td></tr></table></figure><p><strong>页面 UI:</strong> <a target="_blank" rel="noopener" href="http://wom:4040/">http://WOM:4040</a></p><h2 id="2-3-安装-Hadoop-本地单节点"><a href="#2-3-安装-Hadoop-本地单节点" class="headerlink" title="2.3. 安装 Hadoop(本地单节点)"></a>2.3. 安装 Hadoop(本地单节点)</h2><p><strong>安装步骤</strong></p><ul><li>安装 JDK 1.8+</li><li>设置 SSH 无密钥登录</li><li>下载安装 Hadoop</li><li>设置环境变量</li><li>设置 Hadoop 配置文件</li><li>创建并格式化 HDFS 目录</li><li>启动 Hadoop</li><li>打开 Web 页面</li></ul><h3 id="2-3-1-下载安装-Hadoop"><a href="#2-3-1-下载安装-Hadoop" class="headerlink" title="2.3.1. 下载安装 Hadoop"></a>2.3.1. 下载安装 Hadoop</h3><p>官网：<a target="_blank" rel="noopener" href="https://hadoop.apache.org/releases.html">https://hadoop.apache.org/releases.html</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载</span></span><br><span class="line">wget http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz</span><br><span class="line"><span class="comment"># 解压</span></span><br><span class="line">tar -zxvf hadoop-2.7.7.tar.gz</span><br><span class="line"><span class="comment"># 迁移</span></span><br><span class="line">sudo mv hadoop-2.7.7 /usr/<span class="built_in">local</span>/hadoop</span><br><span class="line"><span class="comment"># 授权</span></span><br><span class="line">sudo chown scfan:scfan -R usr/<span class="built_in">local</span>/hadoop</span><br></pre></td></tr></table></figure><h3 id="2-3-2-设置环境变量"><a href="#2-3-2-设置环境变量" class="headerlink" title="2.3.2. 设置环境变量"></a>2.3.2. 设置环境变量</h3><p>文件 /etc/profile</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## hadoop home</span></span><br><span class="line">export HADOOP_HOME=/usr/local/hadoop</span><br><span class="line"><span class="comment"># hadoop path</span></span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/<span class="built_in">bin</span></span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br><span class="line"><span class="comment"># hadoop else env</span></span><br><span class="line">export HADOOP_MAPRED_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_COMMON_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_HDFS_HOME=$HADOOP_HOME</span><br><span class="line">export YARN_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_PREFIX=$HADOOP_HOME</span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line"><span class="comment"># hadoop lib</span></span><br><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native</span><br><span class="line">export HADOOP_OPTS=<span class="string">&quot;-Djava.library.path=$HADOOP_HOME/lib&quot;</span></span><br><span class="line">export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JAVA_LIBRARY_PATH</span><br></pre></td></tr></table></figure><h3 id="2-3-3-修改-Hadoop-配置文件"><a href="#2-3-3-修改-Hadoop-配置文件" class="headerlink" title="2.3.3. 修改 Hadoop 配置文件"></a>2.3.3. 修改 Hadoop 配置文件</h3><p><strong>配置文件：/usr/local/hadoop/etc/hadoop/hadoop-env.sh</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java/jdk1<span class="number">.8</span><span class="number">.0_171</span></span><br></pre></td></tr></table></figure><p><strong>HDFS 默认名称 /usr/local/hadoop/etc/hadoop/core-site.xml</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>MapReduce 配置 /usr/local/hadoop/etc/hadoop/yarn-site.xml</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 后续如果 spark-yarn 部署报错，需要解开此处</span></span><br><span class="line"><span class="comment">    &lt;property&gt;</span></span><br><span class="line"><span class="comment">        &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;</span></span><br><span class="line"><span class="comment">        &lt;value&gt;master:8032&lt;/value&gt;</span></span><br><span class="line"><span class="comment">    &lt;/property&gt;</span></span><br><span class="line"><span class="comment">    &lt;property&gt;</span></span><br><span class="line"><span class="comment">      &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;</span></span><br><span class="line"><span class="comment">      &lt;value&gt;master:8030&lt;/value&gt;</span></span><br><span class="line"><span class="comment">    &lt;/property&gt;</span></span><br><span class="line"><span class="comment">    &lt;property&gt;</span></span><br><span class="line"><span class="comment">      &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;</span></span><br><span class="line"><span class="comment">      &lt;value&gt;master:8031&lt;/value&gt;</span></span><br><span class="line"><span class="comment">    &lt;/property&gt;</span></span><br><span class="line"><span class="comment">    --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>Job 配置 /usr/local/hadoop/etc/hadoop/mapred-site.xml</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">vaule</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">vaule</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>HDFS 分布式文件系统 /usr/local/hadoop/etc/hadoop/hdfs-site.xml</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">vlaue</span>&gt;</span>3<span class="tag">&lt;/<span class="name">vlaue</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">vlaue</span>&gt;</span>file:/usr/local/hadoop/hadoop_data/hdfs/namenode<span class="tag">&lt;/<span class="name">vlaue</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">vlaue</span>&gt;</span>file:/usr/local/hadoop/hadoop_data/hdfs/datanode<span class="tag">&lt;/<span class="name">vlaue</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="2-3-4-格式化目录"><a href="#2-3-4-格式化目录" class="headerlink" title="2.3.4. 格式化目录"></a>2.3.4. 格式化目录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建存储目录</span></span><br><span class="line">mkdir -p /usr/<span class="built_in">local</span>/hadoop/hadoop_data/hdfs/namenode/</span><br><span class="line">mkdir -p /usr/<span class="built_in">local</span>/hadoop/hadoop_data/hdfs/datanode/</span><br><span class="line"><span class="comment"># 进行格式化(如果报错，删除namenode下文件夹current)</span></span><br><span class="line">hadoop namenode -format <span class="comment"># 会删除HDFS数据</span></span><br></pre></td></tr></table></figure><h3 id="2-3-5-查看页面"><a href="#2-3-5-查看页面" class="headerlink" title="2.3.5. 查看页面"></a>2.3.5. 查看页面</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动HDFS</span></span><br><span class="line">start-dfs.sh</span><br><span class="line"><span class="comment"># 启动Yarn</span></span><br><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure><p>Hadoop 界面： <a target="_blank" rel="noopener" href="http://localhost:8088/">http://localhost:8088</a><br>HDFS 界面： <a target="_blank" rel="noopener" href="http://localhost:50070/">http://localhost:50070</a></p><h2 id="2-4-部署-Spark-Standalone-Mode"><a href="#2-4-部署-Spark-Standalone-Mode" class="headerlink" title="2.4. 部署 Spark Standalone Mode"></a>2.4. 部署 Spark Standalone Mode</h2><p>参考链接：</p><ul><li><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/spark-standalone.html">http://spark.apache.org/docs/latest/spark-standalone.html</a></li></ul><p>本地单机模式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动主节点 默认端口8080</span></span><br><span class="line">./sbin/start-master.sh -h localhost --webui-port 8080</span><br><span class="line"><span class="comment"># 启动子节点</span></span><br><span class="line">./sbin/start-slave.sh &lt;master-spark-URL&gt;</span><br><span class="line">例如: &lt;master-spark-URL&gt; 可以在页面localhost:8080上面查看</span><br><span class="line">./sbin/start-slave.sh spark://localhost:7077</span><br></pre></td></tr></table></figure><h2 id="2-5-部署-Spark-Mesos-模式"><a href="#2-5-部署-Spark-Mesos-模式" class="headerlink" title="2.5. 部署 Spark Mesos 模式"></a>2.5. 部署 Spark Mesos 模式</h2><p>参考链接： <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/running-on-mesos.html">http://spark.apache.org/docs/latest/running-on-mesos.html</a></p><p><strong>Mesos 安装</strong><br>参考链接：<a target="_blank" rel="noopener" href="https://open.mesosphere.com/downloads/mesos/">https://open.mesosphere.com/downloads/mesos/</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载系统对应 rpm 包</span></span><br><span class="line">wget http://repos.mesosphere.com/el/6/x86_64/RPMS/mesos-1.7.0-2.0.1.el6.x86_64.rpm</span><br><span class="line">rpm -ivh mesos-1.7.0-2.0.1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure><p><strong>前端 WebUI 启动命令</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mesos master --ip=localhost  --work_dir=/var/lib/mesos</span><br></pre></td></tr></table></figure><p><strong>前端 WebUI 地址:</strong><br><a target="_blank" rel="noopener" href="http://localhost:5050/#/">http://localhost:5050/#/</a></p><h2 id="2-6-部署-Spark-Yarn"><a href="#2-6-部署-Spark-Yarn" class="headerlink" title="2.6. 部署 Spark Yarn"></a>2.6. 部署 Spark Yarn</h2><p>参考链接： <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/running-on-yarn.html">http://spark.apache.org/docs/latest/running-on-yarn.html</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">命令参数:</span><br><span class="line">./bin/spark-submit --class path.to.your.Class --master yarn --deploy-mode cluster [options] &lt;app jar&gt; [app options]</span><br><span class="line">命令样例:</span><br><span class="line">$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --driver-memory 4g \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    --queue thequeue \</span><br><span class="line">    examples/jars/spark-examples*.jar \</span><br><span class="line">    10</span><br></pre></td></tr></table></figure><h3 id="2-6-1-问题记录"><a href="#2-6-1-问题记录" class="headerlink" title="2.6.1. 问题记录"></a>2.6.1. 问题记录</h3><p><strong>问题说明</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 执行命令</span><br><span class="line">$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --driver-memory 4g \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    --queue thequeue \</span><br><span class="line">    examples/jars/spark-examples*.jar \</span><br><span class="line">    10</span><br><span class="line"># 报错如下</span><br><span class="line">2018-12-07 16:19:07 INFO  Client:871 - Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)</span><br></pre></td></tr></table></figure><p><strong>问题解决</strong><br>yarn-site.xml 增加如下内容</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;master:8032&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;master:8030&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;master:8031&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br></pre></td></tr></table></figure><h2 id="2-7-部署-Spark-Kubernetes"><a href="#2-7-部署-Spark-Kubernetes" class="headerlink" title="2.7. 部署 Spark Kubernetes"></a>2.7. 部署 Spark Kubernetes</h2><p><strong>Spark Kubernetes:</strong> https//spark.apache.org/docs/latest/running-on-kubernetes.html</p><p><strong>kubernetes 官网：</strong><a target="_blank" rel="noopener" href="https://kubernetes.io/">https://kubernetes.io/</a></p><p>TODO</p><h1 id="3-Spark-数据统计"><a href="#3-Spark-数据统计" class="headerlink" title="3. Spark 数据统计"></a>3. Spark 数据统计</h1><h2 id="3-1-SparkRDD-使用"><a href="#3-1-SparkRDD-使用" class="headerlink" title="3.1. SparkRDD 使用"></a>3.1. SparkRDD 使用</h2><p>RDD - 弹性分布式数据集</p><p>RDD 是可以并行操作的容错的容错集合。创建 RDD 有两种方法：并行化 驱动程序中的现有集合，或引用外部存储系统中的数据集</p><p>官网 RDD 参考链接：<br><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds">http://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds</a></p><p><strong>Spark 启动</strong><br>bin/pyspark</p><p><strong>Spark 初始化</strong></p><ul><li>创建 SparkContext 对象，告知 Spark 如何访问集群。</li><li>appName 参数是应用程序在群集 UI 上显示的名称</li><li>master 是 URL</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>appName=<span class="string">&quot;fdm&quot;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>master=<span class="string">&quot;mesos://localhost:5050&quot;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>conf = SparkConf().setAppName(appName).setMaster(master)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sc = SparkContext(conf=conf)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sc</span><br><span class="line">&lt;SparkContext master=local[*] appName=PySparkShell&gt;</span><br></pre></td></tr></table></figure><p><strong>并行化集合</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>distData = sc.parallelize(data)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span> distData</span><br><span class="line">ParallelCollectionRDD[<span class="number">0</span>] at parallelize at PythonRDD.scala:<span class="number">195</span></span><br></pre></td></tr></table></figure><p><strong>外部数据集</strong><br>支持导入本地数据集、HDFS://xxxxxx 等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>distFile = sc.textFile(<span class="string">&quot;data.txt&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>distFile</span><br><span class="line">data.txt MapPartitionsRDD[<span class="number">2</span>] at textFile at NativeMethodAccessorImpl.java:<span class="number">0</span></span><br></pre></td></tr></table></figure><p>可写类型：</p><ul><li>int</li><li>float</li><li>double</li><li>bool</li><li>byte</li><li>null</li><li>dict<br><strong>保存和加载 SequenceFiles</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">4</span>)).<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x, <span class="string">&quot;a&quot;</span> * x))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd.saveAsSequenceFile(<span class="string">&quot;path/to/file&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">sorted</span>(sc.sequenceFile(<span class="string">&quot;path/to/file&quot;</span>).collect())</span><br><span class="line">[(<span class="number">1</span>, <span class="string">u&#x27;a&#x27;</span>), (<span class="number">2</span>, <span class="string">u&#x27;aa&#x27;</span>), (<span class="number">3</span>, <span class="string">u&#x27;aaa&#x27;</span>)]</span><br></pre></td></tr></table></figure><h2 id="3-2-SparkDataFrame-使用"><a href="#3-2-SparkDataFrame-使用" class="headerlink" title="3.2. SparkDataFrame 使用"></a>3.2. SparkDataFrame 使用</h2><p>官网 DataFrame 参考链接：<br><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/sql-programming-guide.html">http://spark.apache.org/docs/latest/sql-programming-guide.html</a></p><p><strong>初始化 Spark Session</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(<span class="string">&quot;Python Spark SQL basic example&quot;</span>) \</span><br><span class="line">    .config(<span class="string">&quot;spark.some.config.option&quot;</span>, <span class="string">&quot;some-value&quot;</span>) \</span><br><span class="line">    .getOrCreate()</span><br></pre></td></tr></table></figure><p><strong>创建 DataFrame</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># spark is an existing SparkSession</span></span><br><span class="line">df = spark.read.json(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line"><span class="comment"># Displays the content of the DataFrame to stdout</span></span><br><span class="line">df.show()</span><br><span class="line"><span class="comment"># +----+-------+</span></span><br><span class="line"><span class="comment"># | age|   name|</span></span><br><span class="line"><span class="comment"># +----+-------+</span></span><br><span class="line"><span class="comment"># |null|Michael|</span></span><br><span class="line"><span class="comment"># |  30|   Andy|</span></span><br><span class="line"><span class="comment"># |  19| Justin|</span></span><br><span class="line"><span class="comment"># +----+-------+</span></span><br></pre></td></tr></table></figure><h2 id="3-3-SparkSQL-使用"><a href="#3-3-SparkSQL-使用" class="headerlink" title="3.3. SparkSQL 使用"></a>3.3. SparkSQL 使用</h2><p>官网参考链接：<br><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/sql-distributed-sql-engine.html#running-the-thrift-jdbcodbc-server">http://spark.apache.org/docs/latest/sql-distributed-sql-engine.html#running-the-thrift-jdbcodbc-server</a></p><p><strong>启动 Thrift JDBC / ODBC 服务器</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-thriftserver.sh</span><br></pre></td></tr></table></figure><p><strong>访问前端 UI</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://localhost:4042/SQL/</span><br></pre></td></tr></table></figure><p><strong>使用 beeline 来测试 Thrift JDBC / ODBC 服务器：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./bin/beeline</span><br><span class="line">beeline&gt; !connect jdbc:hive2://localhost:10000</span><br><span class="line"><span class="comment"># 输入用户名和空白密码</span></span><br></pre></td></tr></table></figure><p>启动 spark-sql</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-sql</span><br></pre></td></tr></table></figure><h1 id="Spark-问题整理"><a href="#Spark-问题整理" class="headerlink" title="Spark 问题整理"></a>Spark 问题整理</h1><h2 id="Service-‘SparkUI’-could-not-bind-on-port-4040-Attempting-port-4041"><a href="#Service-‘SparkUI’-could-not-bind-on-port-4040-Attempting-port-4041" class="headerlink" title="Service ‘SparkUI’ could not bind on port 4040. Attempting port 4041."></a>Service ‘SparkUI’ could not bind on port 4040. Attempting port 4041.</h2><p>问题：运行 Spark 脚本报错</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">self.spark = SparkSession.builder.master(self.MASTER).appName(self.APPNAME).getOrCreate()</span><br></pre></td></tr></table></figure><p>原因：<br>由于启动一个 Spark context 时，SparkUI 默认会使用 4040 端口，当 4040 端口被占用时，则尝试使用另外一个端口</p><p>解决步骤：<br>关闭 Spark-Shell 即可</p><p>错误日志:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2018-12-27 09:38:59 WARN  Utils:66 - Service <span class="string">&#x27;SparkUI&#x27;</span> could not <span class="built_in">bind</span> on port 4040. Attempting port 4041.</span><br><span class="line">I1227 09:39:02.612689 26652 sched.cpp:232] Version: 1.7.0</span><br><span class="line">I1227 09:39:02.619974 26650 sched.cpp:336] New master detected at master@192.168.172.70:5050</span><br><span class="line">I1227 09:39:02.620997 26650 sched.cpp:356] No credentials provided. Attempting to register without authentication</span><br></pre></td></tr></table></figure><h1 id="Spark-操作细节"><a href="#Spark-操作细节" class="headerlink" title="Spark 操作细节"></a>Spark 操作细节</h1><h1 id="Mesos-使用"><a href="#Mesos-使用" class="headerlink" title="Mesos 使用"></a>Mesos 使用</h1><h2 id="Messos-安装-amp-配置"><a href="#Messos-安装-amp-配置" class="headerlink" title="Messos 安装&amp;配置"></a>Messos 安装&amp;配置</h2><p><a target="_blank" rel="noopener" href="http://mesos.apache.org/documentation/latest/building/">mesos 官方部署文档</a></p><h2 id="Mesos-启动-amp-关闭"><a href="#Mesos-启动-amp-关闭" class="headerlink" title="Mesos 启动 &amp; 关闭"></a>Mesos 启动 &amp; 关闭</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/spark</span><br><span class="line">./bin/spark-shell --master mesos://192.168.172.70:5050</span><br><span class="line"></span><br><span class="line">/etc/mesos-master</span><br><span class="line">/etc/mesos-slave</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭 mesos-master</span></span><br><span class="line">[root@WOM mesos-master]<span class="comment"># netstat -lntp | grep 5050</span></span><br><span class="line">[root@WOM mesos-master]<span class="comment"># kill -9 XXXX</span></span><br><span class="line"><span class="comment"># 启动 mesoso-master</span></span><br><span class="line">mesos-master --work_dir=/usr/<span class="built_in">local</span>/mesos/master_data --log_dir=/usr/<span class="built_in">local</span>/mesos/master_logs --no-hostname_lookup --ip=192.168.172.70 --cluster=wom</span><br><span class="line"><span class="comment"># 启动master-salve</span></span><br><span class="line">mesos-slave --work_dir=/usr/<span class="built_in">local</span>/mesos/salves_data --log_dir=/usr/<span class="built_in">local</span>/mesos/salves_logs --master=192.168.172.70:5050 --no-hostname_lookup --ip=192.168.172.70 --port=5052</span><br><span class="line"><span class="comment"># 启动 Spark</span></span><br><span class="line">./sbin/start-master.sh -h localhost --webui-port 8080</span><br><span class="line">(env) [scfan@WOM spark]$ bin/spark-shell --master mesos://192.168.172.70:5050 --total-executor-cores 1 --driver-memory 512M --executor-memory 512M</span><br><span class="line"><span class="comment">## 2.4. 部署Spark Standalone Mode</span></span><br><span class="line">参考链接：</span><br><span class="line">- http://spark.apache.org/docs/latest/spark-standalone.html</span><br><span class="line"></span><br><span class="line">本地单机模式</span><br><span class="line"><span class="comment"># 启动主节点 默认端口8080</span></span><br><span class="line">./sbin/start-master.sh -h localhost --webui-port 8080</span><br><span class="line"><span class="comment"># 启动子节点</span></span><br><span class="line">./sbin/start-slave.sh &lt;master-spark-URL&gt;</span><br><span class="line">例如: &lt;master-spark-URL&gt; 可以在页面localhost:8080上面查看</span><br><span class="line">./sbin/start-slave.sh spark://localhost:7077</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="删除-mesos-工作目录"><a href="#删除-mesos-工作目录" class="headerlink" title="删除 mesos 工作目录"></a>删除 mesos 工作目录</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">如果我需要一个新的mesos集群，我需要master的干净工作目录。但问题不在于10.142.55.202约瑟夫吴说。我清除了所有的word_dir，并摆脱了这个问题。</span><br><span class="line"></span><br><span class="line">如何清理工作目录：</span><br><span class="line"></span><br><span class="line">找到mesos-master工作目录</span><br><span class="line"></span><br><span class="line">$ cat /etc/mesos-master/work_dir</span><br><span class="line">/var/lib/mesos</span><br><span class="line">去掉它</span><br><span class="line"></span><br><span class="line">$ rm -rf /var/lib/mesos</span><br></pre></td></tr></table></figure><h2 id="Initial-job-has-not-accepted-any-resources-check-your-cluster-UI-to-ensure-that-workers-are-registered-and-have-sufficient-resources"><a href="#Initial-job-has-not-accepted-any-resources-check-your-cluster-UI-to-ensure-that-workers-are-registered-and-have-sufficient-resources" class="headerlink" title="Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources"></a>Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources</h2><p>当前的集群的可用资源不能满足应用程序所请求的资源</p><p>资源分 2 类： cores 和 ram<br>Core 代表对执行可用的 executor slots<br>Ram 代表每个 Worker 上被需要的空闲内存来运行你的 Application。<br>解决方法：<br>应用不要请求多余空闲可用资源的<br>关闭掉已经执行结束的 Application</p><p>解决方法：</p><ol><li>执行参数修改内存大小</li><li>释放内存，增加内存大小</li></ol><p>export SPARK_WORKER_MEMORY=512M<br>export SPARK_DAEMON_MEMORY=256M</p><p>这些–executor-memory、–driver-memory 你是否能先指定得更小些(比如 50M、100M)</p><p>1.因为提交任务的节点不能和 worker 节点交互，因为提交完任务后提交任务节点上会起一个进程，展示任务进度，大多端口为 4044，工作节点需要反馈进度给该该端口，所以如果主机名或者 IP 在 hosts 中配置不正确。所以检查下主机名和 ip 是否配置正确。</p><p>2.也有可能是内存不足造成的。内存设置可以根据情况调整下。另外，也检查下 web UI 看看，确保 worker 节点处于 alive 状态。</p><h1 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h1><p>tags: <code>202101</code> <code>大数据</code></p><h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看分区目录</span></span><br><span class="line">hdfs://offline-cluster/user/hive/warehouse/db_dwd_test.db/dwd_ei_basic_tsc_tax_illegal_ds/p_date=20200916/part-00109,</span><br><span class="line"><span class="comment"># 删除分区目录数据后，也必须删除要分区</span></span><br><span class="line">alter table db_dwd_test.dwd_ei_basic_tsc_tax_illegal_ds drop partition (p_date=<span class="string">&#x27;20200917&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入 调试环境</span></span><br><span class="line">~/spark/bin/pyspark --master <span class="variable">$&#123;master_ip&#125;</span> --total-executor-cores 5</span><br><span class="line"><span class="comment"># 执行任务</span></span><br><span class="line">~/spark/bin/spark-sql --master &#123;master_ip&#125; --driver-memory 1g --executor-memory 1g --executor-cores 1 --total-executor-cores 2 xx.py</span><br></pre></td></tr></table></figure><h2 id="问题记录"><a href="#问题记录" class="headerlink" title="问题记录"></a>问题记录</h2><h3 id="分区数量过多"><a href="#分区数量过多" class="headerlink" title="分区数量过多"></a>分区数量过多</h3><p>a union all b<br>最后的分区数量会变为 a 的分区+b 的分区</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">20/09/18 14:12:13 INFO TaskSetManager: Finished task 17432.0 <span class="keyword">in</span> stage 3.0 (TID 17437) <span class="keyword">in</span> 503 ms on 192.168.201.8 (executor 1) (17435/17436)</span><br><span class="line">20/09/18 14:12:13 INFO TaskSetManager: Finished task 17426.0 <span class="keyword">in</span> stage 3.0 (TID 17431) <span class="keyword">in</span> 1312 ms on 192.168.207.96 (executor 0) (17436/17436)</span><br><span class="line">20/09/18 14:12:13 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool</span><br></pre></td></tr></table></figure><h3 id="spark-submit-报-No-module-的错误"><a href="#spark-submit-报-No-module-的错误" class="headerlink" title="spark-submit 报 No module 的错误"></a>spark-submit 报 No module 的错误</h3><p>问题说明：使用外部依赖包，报 not module xxxx</p><p>可能原因 <a target="_blank" rel="noopener" href="https://segmentfault.com/q/1010000004569365">https://segmentfault.com/q/1010000004569365</a></p><p>问题：No module named tools.utils</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[An error occurred <span class="keyword">while</span> calling o35.sql.</span><br><span class="line">: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 <span class="keyword">in</span> stage 0.0 failed 4 <span class="built_in">times</span>, most recent failure: Lost task 5.3 <span class="keyword">in</span> stage 0.0 (TID 43, 192.168.201.40, executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;/home/dfs/spark/python/lib/pyspark.zip/pyspark/worker.py&quot;</span>, line 159, <span class="keyword">in</span> main</span><br><span class="line">    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile)</span><br><span class="line">  File <span class="string">&quot;/home/dfs/spark/python/lib/pyspark.zip/pyspark/worker.py&quot;</span>, line 93, <span class="keyword">in</span> read_udfs</span><br><span class="line">    arg_offsets, udf = read_single_udf(pickleSer, infile)</span><br><span class="line">  File <span class="string">&quot;/home/dfs/spark/python/lib/pyspark.zip/pyspark/worker.py&quot;</span>, line 79, <span class="keyword">in</span> read_single_udf</span><br><span class="line">    f, return_type = read_command(pickleSer, infile)</span><br><span class="line">  File <span class="string">&quot;/home/dfs/spark/python/lib/pyspark.zip/pyspark/worker.py&quot;</span>, line 55, <span class="keyword">in</span> read_command</span><br><span class="line">    <span class="built_in">command</span> = serializer._read_with_length(file)</span><br><span class="line">  File <span class="string">&quot;/home/dfs/spark/python/lib/pyspark.zip/pyspark/serializers.py&quot;</span>, line 169, <span class="keyword">in</span> _read_with_length</span><br><span class="line">    <span class="built_in">return</span> self.loads(obj)</span><br><span class="line">  File <span class="string">&quot;/home/dfs/spark/python/lib/pyspark.zip/pyspark/serializers.py&quot;</span>, line 458, <span class="keyword">in</span> loads</span><br><span class="line">    <span class="built_in">return</span> pickle.loads(obj)</span><br><span class="line">  File <span class="string">&quot;./scripts.zip/scripts/zhengfujigou.py&quot;</span>, line 19, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">ImportError: No module named tools.utils</span><br></pre></td></tr></table></figure><p>修改步骤 1：替换 <code>--py-files</code> 为单个</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">    bash /home/scrapyer/spark/bin/spark-submit --master <span class="variable">$&#123;spark_ip&#125;</span> \</span><br><span class="line">        --executor-memory 1g \</span><br><span class="line">        --total-executor-cores 20 \</span><br><span class="line">        --py-files /home/scrapyer/workspace/fansichao/workspace/<span class="variable">$&#123;frame_dir&#125;</span>/conf.zip \</span><br><span class="line">        --py-files /home/scrapyer/workspace/fansichao/workspace/<span class="variable">$&#123;frame_dir&#125;</span>/tools.zip \</span><br><span class="line">        --py-files /home/scrapyer/workspace/fansichao/workspace/<span class="variable">$&#123;frame_dir&#125;</span>/scripts.zip \</span><br><span class="line">        <span class="variable">$&#123;code_path&#125;</span> <span class="variable">$&#123;conf_path&#125;</span> <span class="variable">$&#123;p_date&#125;</span> &gt; logs/<span class="variable">$&#123;app_name&#125;</span>_<span class="variable">$&#123;p_date&#125;</span>.<span class="built_in">log</span> 2&gt;&amp;1</span><br><span class="line"></span><br><span class="line">修改为</span><br><span class="line"></span><br><span class="line">    bash /home/scrapyer/spark/bin/spark-submit --master <span class="variable">$&#123;spark_ip&#125;</span> \</span><br><span class="line">        --executor-memory 1g \</span><br><span class="line">        --total-executor-cores 5 \</span><br><span class="line">        --py-files conf.zip,tools.zip,scripts.zip \</span><br><span class="line">        <span class="variable">$&#123;code_path&#125;</span> <span class="variable">$&#123;conf_path&#125;</span> <span class="variable">$&#123;p_date&#125;</span></span><br><span class="line"><span class="comment">#        &gt; logs/$&#123;app_name&#125;_$&#123;p_date&#125;.log 2&gt;&amp;1</span></span><br></pre></td></tr></table></figure><p>使用同一个 <code>--py-files</code>, 而非多个<code>--py-files</code> !!!</p><p>多个 <code>--py-files</code> 在 Thanos 平台上无法真实 Kill 程序。</p><h2 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h2><p>Spark 官方文档</p><ul><li><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/quick-start.html">http://spark.apache.org/docs/latest/quick-start.html</a></li><li><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/rdd-programming-guide.html">http://spark.apache.org/docs/latest/rdd-programming-guide.html</a></li><li><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/sql-programming-guide.html">http://spark.apache.org/docs/latest/sql-programming-guide.html</a></li></ul><h1 id="技术笔记"><a href="#技术笔记" class="headerlink" title="技术笔记"></a>技术笔记</h1><h1><a href="#" class="headerlink"></a></h1><h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><p>Vscode 配置代码行数<br>editor.rulers，默认 79,建议 160</p><p>– Spark 提交任务的三种方式<br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/itboys/p/9998666.html">https://www.cnblogs.com/itboys/p/9998666.html</a></p><h2 id="参考资源"><a href="#参考资源" class="headerlink" title="参考资源"></a>参考资源</h2><p>优质博客</p><ul><li>Python2 爬虫学习系列教程 <a target="_blank" rel="noopener" href="https://cuiqingcai.com/1052.html">https://cuiqingcai.com/1052.html</a></li></ul><h1 id="Spark-技术知识"><a href="#Spark-技术知识" class="headerlink" title="Spark 技术知识"></a>Spark 技术知识</h1><h2 id="Spark-知识术语"><a href="#Spark-知识术语" class="headerlink" title="Spark 知识术语"></a>Spark 知识术语</h2><p>Spark 分区的区别</p><p>Spark 学习目录</p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/a544258023/article/details/94635807">https://blog.csdn.net/a544258023/article/details/94635807</a></p><h2 id="常用命令-1"><a href="#常用命令-1" class="headerlink" title="常用命令"></a>常用命令</h2><p>Spark - 重新分区</p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u010720408/article/details/90229461">https://blog.csdn.net/u010720408/article/details/90229461</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看分区目录</span></span><br><span class="line"> hdfs://offline-cluster/user/hive/warehouse/db_dwd_test.db/dwd_ei_basic_tsc_tax_illegal_ds/p_date=20200916/part-00109,</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hive中执行 删除分区目录数据后，也必须删除要分区</span></span><br><span class="line">alter table db_dwd_test.dwd_ei_basic_tsc_tax_illegal_ds drop partition (p_date=<span class="string">&#x27;20200917&#x27;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="多个-py-files-使用"><a href="#多个-py-files-使用" class="headerlink" title="多个 py-files 使用"></a>多个 py-files 使用</h3><p>参考链接：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/92be93cfbb97">https://www.jianshu.com/p/92be93cfbb97</a></p><p>hivE 数据类型 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/dangjf/p/10071683.html">https://www.cnblogs.com/dangjf/p/10071683.html</a></p><p>Spark 数据类型转换 <a target="_blank" rel="noopener" href="https://blog.csdn.net/an1090239782/article/details/102541024">https://blog.csdn.net/an1090239782/article/details/102541024</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">ByteType：代表一个字节的整数。范围是-<span class="number">128</span>到<span class="number">127</span></span><br><span class="line">ShortType：代表两个字节的整数。范围是-<span class="number">32768</span>到<span class="number">32767</span></span><br><span class="line">IntegerType：代表<span class="number">4</span>个字节的整数。范围是-<span class="number">2147483648</span>到<span class="number">2147483647</span></span><br><span class="line">LongType：代表<span class="number">8</span>个字节的整数。范围是-<span class="number">9223372036854775808</span>到<span class="number">9223372036854775807</span></span><br><span class="line">FloatType：代表<span class="number">4</span>字节的单精度浮点数 DoubleType：代表<span class="number">8</span>字节的双精度浮点数</span><br><span class="line">DecimalType：代表任意精度的<span class="number">10</span>进制数据。通过内部的java.math.BigDecimal支持。BigDecimal由一个任意精度的整型非标度值和一个<span class="number">32</span>位整数组成</span><br><span class="line">StringType：代表一个字符串值</span><br><span class="line">BinaryType：代表一个byte序列值</span><br><span class="line">BooleanType：代表boolean值</span><br><span class="line">Datetime类型：</span><br><span class="line"></span><br><span class="line">TimestampType：代表包含字段年，月，日，时，分，秒的值</span><br><span class="line">DateType：代表包含字段年，月，日的值</span><br></pre></td></tr></table></figure><h2 id="Spark-问题记录"><a href="#Spark-问题记录" class="headerlink" title="Spark 问题记录"></a>Spark 问题记录</h2><h3 id="TaskSetManager-过多，分区数过多"><a href="#TaskSetManager-过多，分区数过多" class="headerlink" title="TaskSetManager 过多，分区数过多"></a>TaskSetManager 过多，分区数过多</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 问题背景</span></span><br><span class="line">sql = <span class="string">&quot;&quot;&quot;insert overwrite table &#123;dwd_table&#125; partition(p_date=&#x27;&#123;p_date&#125;&#x27;)</span></span><br><span class="line"><span class="string">&#123;dwd_table&#125; union &#123;ods_table&#125;&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 问题描述</span></span><br><span class="line">发现sql执行时，TaskSetManager分配的任务过多，part过多</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解决方案</span></span><br><span class="line"><span class="number">1.</span> 在读取HDFS的时候设置固定的分区数。 数据进入ods时配置好分区数</span><br><span class="line">rd = spark_tools.spark_context.textFile(hdfs_path).repartition(<span class="number">200</span>)</span><br><span class="line"><span class="number">2.</span> 此sql执行方法修改，先保存为df,再进行df.repartition(<span class="number">200</span>, <span class="string">&quot;p_date&quot;</span>)重新分区后,再保存到数据中。</span><br><span class="line"></span><br><span class="line"><span class="comment"># 其他说明</span></span><br><span class="line">去除 union, 使用 <span class="string">&quot;insert overwrite table &#123;dwd_table&#125;&quot;</span> <span class="keyword">and</span> <span class="string">&quot;insert into table &#123;dwd_table&#125;&quot;</span></span><br><span class="line">时，会产生 copy 分区，此方法不适用！</span><br><span class="line"></span><br><span class="line">db_dwd_test.db/dwd_ei_basic_tsc_tax_illegal_ds/p_date=<span class="number">20200917</span>/part-00199</span><br><span class="line">db_dwd_test.db/dwd_ei_basic_tsc_tax_illegal_ds/p_date=<span class="number">20200917</span>/part-00199_copy_1</span><br></pre></td></tr></table></figure><p>详细日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">20/09/18 14:12:13 INFO TaskSetManager: Finished task 17432.0 <span class="keyword">in</span> stage 3.0 (TID 17437) <span class="keyword">in</span> 503 ms on 192.168.201.8 (executor 1) (17435/17436)</span><br><span class="line">20/09/18 14:12:13 INFO TaskSetManager: Finished task 17426.0 <span class="keyword">in</span> stage 3.0 (TID 17431) <span class="keyword">in</span> 1312 ms on 192.168.207.96 (executor 0) (17436/17436)</span><br><span class="line">20/09/18 14:12:13 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool</span><br><span class="line">20/09/18 14:12:13 INFO DAGScheduler: ResultStage 3 (sql at NativeMethodAccessorImpl.java:0) finished <span class="keyword">in</span> 1248.035 s</span><br><span class="line">20/09/18 14:12:13 INFO DAGScheduler: Job 2 finished: sql at NativeMethodAccessorImpl.java:0, took 1250.992204 s</span><br><span class="line"></span><br><span class="line">-06729, dest: hdfs://offline-cluster/user/hive/warehouse/db_dwd_test.db/dwd_ei_basic_tsc_tax_illegal_ds/p_date=20200917/part-06729, Status:<span class="literal">true</span></span><br><span class="line">20/09/18 14:20:15 INFO Hive: Renaming src: hdfs://offline-cluster/user/hive/warehouse/db_dwd_test.db/dwd_ei_basic_tsc_tax_illegal_ds/.hive-staging_hive_2020-09-18_13-51-12_491_7780753504198483023-1/-ext-10000/part-06730, dest: hdfs://offline-cluster/user/hive/warehouse/db_dwd_test.db/dwd_ei_basic_tsc_tax_illegal_ds/p_date=20200917/part-06730, Status:<span class="literal">true</span></span><br><span class="line">20/09/18 14:20:15 INFO Hive: Renaming src: hdfs://offline-cluster/user/hive/warehouse/db_dwd_test.db/dwd_ei_basic_tsc_tax_illegal_ds/.hive-staging_hive_2020-09-18_13-51-12_491_7780753504198483023-1/-ext-10000/part-06731, dest: hdfs://offline-cluster/user/hive/warehouse/db_dwd_test.db/dwd_ei_basic_tsc_tax_illegal_ds/p_date=20200917/part-06731, Status:<span class="literal">true</span></span><br></pre></td></tr></table></figure></div><div class="article-footer"><blockquote class="mt-2x"><ul class="post-copyright list-unstyled"><li class="post-copyright-link hidden-xs"><strong>本文链接：</strong> <a href="http://fansichao.github.com/blog/Tools/BigData/bigdata-spark.md/" title="Spark-使用文档" target="_blank" rel="external">http://fansichao.github.com/blog/Tools/BigData/bigdata-spark.md/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！</li></ul></blockquote><div class="panel panel-default panel-badger"><div class="panel-body"><figure class="media"><div class="media-left"><a href="https://github.com/fansichao" target="_blank" class="img-burn thumb-sm visible-lg"><img src="/blog/images/avatar.jpg" class="img-rounded w-full" alt=""></a></div><div class="media-body"><h3 class="media-heading"><a href="https://github.com/fansichao" target="_blank"><span class="text-dark">fansichao</span><small class="ml-1x">Data Engineer &amp; SSE &amp; CYO</small></a></h3><div>个人简介。</div></div></figure></div></div></div></article><section id="comments"><div id="vcomments"></div></section></div><nav class="bar bar-footer clearfix" data-stick-bottom><div class="bar-inner"><ul class="pager pull-left"><li class="prev"><a href="/blog/Tools/BigData/bigdata-scylladb.md/" title="ScyllaDB-使用文档"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;Newer</span></a></li><li class="next"><a href="/blog/Tools/BigData/bigdata-prestodb.md/" title="PrestoDB-SQL"><span>Older&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a></li></ul><button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>$</span></button><div class="bar-right"><div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div></div></div></nav><div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog"><div class="modal-dialog" role="document"><div class="modal-content donate"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button><div class="modal-body"><div class="donate-box"><div class="donate-head"><p>Maybe you could buy me a cup of coffee.</p></div><div class="tab-content"><div role="tabpanel" class="tab-pane fade active in" id="alipay"><div class="donate-payimg"><img src="/blog/images/donate/alipayimg.png" alt="Scan Qrcode" title="Scan"></div><p class="text-muted mv">Scan this qrcode</p><p class="text-grey">Open alipay app scan this qrcode, buy me a coffee!</p></div><div role="tabpanel" class="tab-pane fade" id="wechatpay"><div class="donate-payimg"><img src="/blog/images/donate/wechatpayimg.png" alt="Scan Qrcode" title="Scan"></div><p class="text-muted mv">Scan this qrcode</p><p class="text-grey">Open wechat app scan this qrcode, buy me a coffee!</p></div></div><div class="donate-footer"><ul class="nav nav-tabs nav-justified" role="tablist"><li role="presentation" class="active"><a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> alipay</a></li><li role="presentation"><a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> wechat payment</a></li></ul></div></div></div></div></div></div></main><footer class="footer" itemscope itemtype="http://schema.org/WPFooter"><ul class="social-links"><li><a href="https://github.com/cofess" target="_blank" title="Github" data-toggle="tooltip" data-placement="top"><i class="icon icon-github"></i></a></li><li><a href="http://weibo.com/cofess" target="_blank" title="Weibo" data-toggle="tooltip" data-placement="top"><i class="icon icon-weibo"></i></a></li><li><a href="https://twitter.com/iwebued" target="_blank" title="Twitter" data-toggle="tooltip" data-placement="top"><i class="icon icon-twitter"></i></a></li><li><a href="https://www.behance.net/cofess" target="_blank" title="Behance" data-toggle="tooltip" data-placement="top"><i class="icon icon-behance"></i></a></li><li><a href="/blog/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul><div class="copyright"><div class="publishby">Theme by <a href="https://github.com/cofess" target="_blank">cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.</div></div></footer><script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script><script>window.jQuery||document.write('<script src="js/jquery.min.js"><\/script>')</script><script src="/blog/js/plugin.min.js"></script><script src="/blog/js/application.js"></script><script>window.INSIGHT_CONFIG={TRANSLATION:{POSTS:"Posts",PAGES:"Pages",CATEGORIES:"Categories",TAGS:"Tags",UNTITLED:"(Untitled)"},ROOT_URL:"/blog/",CONTENT_URL:"/blog/content.json"}</script><script src="/blog/js/insight.js"></script><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/npm/valine"></script><script type="text/javascript">var GUEST=["nick","mail","link"],meta=(meta="nick,mail,link").split(",").filter(function(e){return-1<GUEST.indexOf(e)});new Valine({el:"#vcomments",verify:!1,notify:!1,appId:"",appKey:"",placeholder:"Just go go",avatar:"mm",meta:meta,pageSize:"10",visitor:!1})</script><script src="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.3.5/dist/jquery.fancybox.min.js"></script><script>$(document).ready(function(){$("article img").not("[hidden]").not(".panel-body img").each(function(){var a,t,n=$(this),e=n.attr("alt"),r=n.parent("a");r.length<1&&(-1!=(t=(a=this.getAttribute("src")).lastIndexOf("?"))&&(a=a.substring(0,t)),r=n.wrap('<a href="'+a+'"></a>').parent("a")),r.attr("data-fancybox","images"),e&&r.attr("data-caption",e)}),$().fancybox({selector:'[data-fancybox="images"]',hash:!1,loop:!1})})</script></body></html>